#!/usr/bin/env python3
"""
Component Exploitation Plugin - Professional Confidence Calculator

Evidence-based confidence calculation system for component exploitation findings.
Implements sophisticated confidence scoring that considers pattern reliability,
context relevance, validation coverage, and cross-verification results.
"""

import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from .data_structures import (
    ComponentType, ExploitationRisk, ComponentVulnerability,
    ExploitationCommand, SecurityAssessment
)

logger = logging.getLogger(__name__)

@dataclass
class ConfidenceEvidence:
    """Evidence factors for confidence calculation."""
    pattern_reliability: float = 0.0
    context_relevance: float = 0.0
    validation_coverage: float = 0.0
    component_exposure: float = 0.0
    permission_analysis: float = 0.0
    exploitation_feasibility: float = 0.0

class ComponentExploitationConfidenceCalculator:
    """
    confidence calculation system for component exploitation analysis.
    
    Implements evidence-based confidence scoring with multi-factor analysis
    considering pattern reliability, context relevance, and exploitation feasibility.
    """
    
    def __init__(self):
        """Initialize confidence calculator with evidence weights and pattern reliability."""
        # Evidence weight factors for component exploitation analysis
        self.evidence_weights = {
            'pattern_reliability': 0.25,      # 25% - Pattern match reliability
            'context_relevance': 0.20,        # 20% - Context relevance
            'validation_coverage': 0.20,      # 20% - Validation coverage
            'component_exposure': 0.15,       # 15% - Component exposure level
            'permission_analysis': 0.10,      # 10% - Permission analysis quality
            'exploitation_feasibility': 0.10  # 10% - Exploitation feasibility
        }
        
        # Pattern reliability database with historical false positive rates
        self.pattern_reliability_data = {
            'explicit_export_detection': {'reliability': 0.95, 'fp_rate': 0.05},
            'permission_protection_analysis': {'reliability': 0.90, 'fp_rate': 0.10},
            'intent_filter_analysis': {'reliability': 0.85, 'fp_rate': 0.15},
            'manifest_parsing': {'reliability': 0.92, 'fp_rate': 0.08},
            'component_discovery': {'reliability': 0.88, 'fp_rate': 0.12},
            'adb_command_generation': {'reliability': 0.93, 'fp_rate': 0.07},
            'risk_assessment': {'reliability': 0.87, 'fp_rate': 0.13},
            'vulnerability_mapping': {'reliability': 0.85, 'fp_rate': 0.15},
            'cvss_calculation': {'reliability': 0.82, 'fp_rate': 0.18},
            'threat_assessment': {'reliability': 0.89, 'fp_rate': 0.11}
        }
        
        # Context factors for confidence adjustment
        self.context_factors = {
            'production_manifest': 1.0,       # High relevance
            'debug_manifest': 0.8,           # Medium-high relevance
            'test_configuration': 0.6,       # Medium relevance
            'sample_application': 0.4,       # Lower relevance
            'malformed_manifest': 0.2        # Lowest relevance
        }
        
        # Component type risk multipliers
        self.component_risk_multipliers = {
            ComponentType.PROVIDER: 1.2,     # Highest risk
            ComponentType.SERVICE: 1.1,      # High risk
            ComponentType.RECEIVER: 1.0,     # Medium risk
            ComponentType.ACTIVITY: 0.9      # Lower risk
        }
        
        # Exploitation feasibility factors
        self.exploitation_factors = {
            'exported_no_permission': 0.95,
            'exported_weak_permission': 0.85,
            'exported_strong_permission': 0.4,
            'not_exported': 0.1
        }
        
        logger.info("Initialized ComponentExploitationConfidenceCalculator")
    
    def calculate_vulnerability_confidence(self, vulnerability: ComponentVulnerability, 
                                         evidence: Dict[str, Any]) -> float:
        """
        Calculate evidence-based confidence for a component vulnerability.
        
        Args:
            vulnerability: Component vulnerability finding
            evidence: Evidence data for confidence calculation
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        try:
            # Extract evidence factors
            evidence_factors = self._extract_evidence_factors(vulnerability, evidence)
            
            # Calculate weighted confidence score
            confidence = (
                evidence_factors.pattern_reliability * self.evidence_weights['pattern_reliability'] +
                evidence_factors.context_relevance * self.evidence_weights['context_relevance'] +
                evidence_factors.validation_coverage * self.evidence_weights['validation_coverage'] +
                evidence_factors.component_exposure * self.evidence_weights['component_exposure'] +
                evidence_factors.permission_analysis * self.evidence_weights['permission_analysis'] +
                evidence_factors.exploitation_feasibility * self.evidence_weights['exploitation_feasibility']
            )
            
            # Apply component type multiplier
            component_multiplier = self.component_risk_multipliers.get(
                vulnerability.component_type, 1.0
            )
            confidence *= component_multiplier
            
            # Apply severity adjustment
            confidence = self._apply_severity_adjustment(confidence, vulnerability.severity)
            
            # Ensure confidence is within valid bounds
            confidence = max(0.0, min(1.0, confidence))
            
            logger.debug(f"Calculated confidence {confidence:.3f} for vulnerability {vulnerability.id}")
            return confidence
            
        except Exception as e:
            logger.error(f"Error calculating vulnerability confidence: {e}")
            return 0.5  # Conservative default
    
    def calculate_command_confidence(self, command: ExploitationCommand, 
                                   evidence: Dict[str, Any]) -> float:
        """
        Calculate confidence for an exploitation command.
        
        Args:
            command: Exploitation command
            evidence: Evidence data for confidence calculation
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        try:
            # Base confidence from pattern reliability
            pattern_type = self._identify_command_pattern(command.command)
            pattern_reliability = self.pattern_reliability_data.get(
                pattern_type, {'reliability': 0.7}
            )['reliability']
            
            # Adjust for component exposure
            exposure_factor = self._assess_component_exposure(command, evidence)
            
            # Adjust for command safety
            safety_factor = self._assess_command_safety_factor(command)
            
            # Calculate overall confidence
            confidence = (
                pattern_reliability * 0.4 +
                exposure_factor * 0.3 +
                safety_factor * 0.3
            )
            
            # Apply risk level adjustment
            confidence = self._apply_risk_adjustment(confidence, command.risk_level)
            
            return max(0.0, min(1.0, confidence))
            
        except Exception as e:
            logger.error(f"Error calculating command confidence: {e}")
            return 0.5  # Conservative default
    
    def calculate_assessment_confidence(self, assessment: SecurityAssessment,
                                      evidence: Dict[str, Any]) -> float:
        """
        Calculate confidence for overall security assessment.
        
        Args:
            assessment: Security assessment results
            evidence: Evidence data for confidence calculation
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        try:
            # Factor 1: Analysis completeness
            completeness = self._assess_analysis_completeness(assessment, evidence)
            
            # Factor 2: Component coverage
            coverage = self._assess_component_coverage(assessment, evidence)
            
            # Factor 3: Validation depth
            validation_depth = self._assess_validation_depth(evidence)
            
            # Factor 4: Cross-validation consistency
            consistency = self._assess_cross_validation_consistency(evidence)
            
            # Calculate weighted confidence
            confidence = (
                completeness * 0.3 +
                coverage * 0.25 +
                validation_depth * 0.25 +
                consistency * 0.2
            )
            
            return max(0.0, min(1.0, confidence))
            
        except Exception as e:
            logger.error(f"Error calculating assessment confidence: {e}")
            return 0.5  # Conservative default
    
    def _extract_evidence_factors(self, vulnerability: ComponentVulnerability,
                                evidence: Dict[str, Any]) -> ConfidenceEvidence:
        """Extract evidence factors for confidence calculation."""
        # Pattern reliability based on detection method
        pattern_type = evidence.get('detection_method', 'component_discovery')
        pattern_reliability = self.pattern_reliability_data.get(
            pattern_type, {'reliability': 0.7}
        )['reliability']
        
        # Context relevance based on file type and location
        context_type = evidence.get('context_type', 'production_manifest')
        context_relevance = self.context_factors.get(context_type, 0.5)
        
        # Validation coverage based on validation methods used
        validation_methods = evidence.get('validation_methods', [])
        validation_coverage = min(1.0, len(validation_methods) * 0.25)
        
        # Component exposure based on export status and permissions
        component_exposure = self._assess_component_exposure_level(vulnerability, evidence)
        
        # Permission analysis quality
        permission_analysis = self._assess_permission_analysis_quality(evidence)
        
        # Exploitation feasibility
        exploitation_feasibility = self._assess_exploitation_feasibility(vulnerability, evidence)
        
        return ConfidenceEvidence(
            pattern_reliability=pattern_reliability,
            context_relevance=context_relevance,
            validation_coverage=validation_coverage,
            component_exposure=component_exposure,
            permission_analysis=permission_analysis,
            exploitation_feasibility=exploitation_feasibility
        )
    
    def _assess_component_exposure_level(self, vulnerability: ComponentVulnerability,
                                       evidence: Dict[str, Any]) -> float:
        """Assess component exposure level for confidence calculation."""
        component_data = evidence.get('component_data', {})
        
        if not component_data.get('exported', False):
            return 0.1  # Not exported, low exposure
        
        permissions = component_data.get('permissions', [])
        if not permissions:
            return 0.95  # Exported with no permissions, high exposure
        elif any(p.startswith('android.permission.') and 'dangerous' in p for p in permissions):
            return 0.4   # Protected by dangerous permissions
        elif any('signature' in p for p in permissions):
            return 0.3   # Protected by signature permissions
        else:
            return 0.7   # Some permission protection
    
    def _assess_permission_analysis_quality(self, evidence: Dict[str, Any]) -> float:
        """Assess quality of permission analysis."""
        permission_data = evidence.get('permission_analysis', {})
        
        if not permission_data:
            return 0.3  # No permission analysis
        
        quality_score = 0.5
        
        # Check for comprehensive permission analysis
        if permission_data.get('custom_permissions_analyzed'):
            quality_score += 0.2
        if permission_data.get('dangerous_permissions_identified'):
            quality_score += 0.2
        if permission_data.get('protection_level_analyzed'):
            quality_score += 0.1
        
        return min(1.0, quality_score)
    
    def _assess_exploitation_feasibility(self, vulnerability: ComponentVulnerability,
                                       evidence: Dict[str, Any]) -> float:
        """Assess exploitation feasibility for confidence calculation."""
        component_data = evidence.get('component_data', {})
        
        if not component_data.get('exported', False):
            return self.exploitation_factors['not_exported']
        
        permissions = component_data.get('permissions', [])
        if not permissions:
            return self.exploitation_factors['exported_no_permission']
        elif any('signature' in str(p) for p in permissions):
            return self.exploitation_factors['exported_strong_permission']
        else:
            return self.exploitation_factors['exported_weak_permission']
    
    def _apply_severity_adjustment(self, confidence: float, severity: str) -> float:
        """Apply severity-based adjustment to confidence."""
        severity_adjustments = {
            'CRITICAL': 1.1,
            'HIGH': 1.05,
            'MEDIUM': 1.0,
            'LOW': 0.95,
            'INFO': 0.9
        }
        
        adjustment = severity_adjustments.get(severity, 1.0)
        return confidence * adjustment
    
    def _apply_risk_adjustment(self, confidence: float, risk_level: ExploitationRisk) -> float:
        """Apply risk level adjustment to confidence."""
        risk_adjustments = {
            ExploitationRisk.CRITICAL: 1.1,
            ExploitationRisk.HIGH: 1.05,
            ExploitationRisk.MEDIUM: 1.0,
            ExploitationRisk.LOW: 0.95,
            ExploitationRisk.MINIMAL: 0.9
        }
        
        adjustment = risk_adjustments.get(risk_level, 1.0)
        return confidence * adjustment
    
    def _identify_command_pattern(self, command: str) -> str:
        """Identify command pattern type for reliability assessment."""
        if 'am start' in command:
            return 'adb_command_generation'
        elif 'am startservice' in command:
            return 'adb_command_generation'
        elif 'am broadcast' in command:
            return 'adb_command_generation'
        elif 'content query' in command:
            return 'adb_command_generation'
        else:
            return 'component_discovery'
    
    def _assess_component_exposure(self, command: ExploitationCommand,
                                 evidence: Dict[str, Any]) -> float:
        """Assess component exposure for command confidence."""
        component_data = evidence.get('component_data', {})
        
        if component_data.get('exported', False):
            if not component_data.get('permissions', []):
                return 0.9  # High exposure
            else:
                return 0.6  # Medium exposure
        else:
            return 0.2  # Low exposure
    
    def _assess_command_safety_factor(self, command: ExploitationCommand) -> float:
        """Assess command safety factor for confidence."""
        safety_factors = {
            'SAFE': 0.9,
            'CAUTION': 0.7,
            'DANGEROUS': 0.5
        }
        
        return safety_factors.get(command.safety_level.value, 0.7)
    
    def _assess_analysis_completeness(self, assessment: SecurityAssessment,
                                    evidence: Dict[str, Any]) -> float:
        """Assess completeness of analysis for confidence calculation."""
        completeness_score = 0.5
        
        # Check various analysis aspects
        if evidence.get('manifest_parsed'):
            completeness_score += 0.15
        if evidence.get('components_discovered'):
            completeness_score += 0.15
        if evidence.get('permissions_analyzed'):
            completeness_score += 0.1
        if evidence.get('intent_filters_analyzed'):
            completeness_score += 0.1
        
        return min(1.0, completeness_score)
    
    def _assess_component_coverage(self, assessment: SecurityAssessment,
                                 evidence: Dict[str, Any]) -> float:
        """Assess component coverage for confidence calculation."""
        total_components = assessment.total_components
        analyzed_components = evidence.get('analyzed_component_count', 0)
        
        if total_components == 0:
            return 0.5
        
        coverage_ratio = analyzed_components / total_components
        return min(1.0, coverage_ratio)
    
    def _assess_validation_depth(self, evidence: Dict[str, Any]) -> float:
        """Assess validation depth for confidence calculation."""
        validation_methods = evidence.get('validation_methods', [])
        return min(1.0, len(validation_methods) * 0.2)
    
    def _assess_cross_validation_consistency(self, evidence: Dict[str, Any]) -> float:
        """Assess cross-validation consistency for confidence calculation."""
        validation_results = evidence.get('validation_results', {})
        
        if not validation_results:
            return 0.5
        
        # Calculate consistency across validation methods
        consistency_scores = []
        for method, result in validation_results.items():
            if isinstance(result, dict) and 'confidence' in result:
                consistency_scores.append(result['confidence'])
        
        if not consistency_scores:
            return 0.5
        
        # Calculate standard deviation to assess consistency
        import statistics
        if len(consistency_scores) > 1:
            std_dev = statistics.stdev(consistency_scores)
            consistency = max(0.0, 1.0 - std_dev)
        else:
            consistency = consistency_scores[0]
        
        return consistency 