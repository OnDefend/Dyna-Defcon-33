"""
Data Structures for Library Vulnerability Scanner

This module contains all data structures, enums, and type definitions
used across the library vulnerability scanner components.

Features:
- Type-safe vulnerability and library information structures
- Enumerated types for consistent categorization
- Structured configuration classes
- Validation and helper methods
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set, Any
from enum import Enum
from datetime import datetime

class VulnerabilitySeverity(Enum):
    """Vulnerability severity levels."""
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    INFO = "INFO"

class DetectionMethod(Enum):
    """Methods used for library detection."""
    GRADLE_ANALYSIS = "gradle_analysis"
    BINARY_ANALYSIS = "binary_analysis"
    SOURCE_ANALYSIS = "source_analysis"
    MANIFEST_ANALYSIS = "manifest_analysis"
    DEPENDENCY_ANALYSIS = "dependency_analysis"

class LibraryType(Enum):
    """Types of libraries that can be detected."""
    CRYPTOGRAPHIC = "cryptographic"
    NETWORKING = "networking"
    IMAGE_PROCESSING = "image_processing"
    COMPRESSION = "compression"
    UTILITY = "utility"
    FRAMEWORK = "framework"
    NATIVE = "native"
    JAVA = "java"

class ExploitabilityLevel(Enum):
    """Levels of vulnerability exploitability."""
    REMOTE = "remote"
    LOCAL = "local"
    PHYSICAL = "physical"
    DIFFICULT = "difficult"
    THEORETICAL = "theoretical"

@dataclass
class LibraryVulnerability:
    """Represents a library vulnerability finding with comprehensive details."""
    
    # Core identification
    library_name: str
    version: str
    vulnerability_id: str
    cve_id: str
    
    # Severity and confidence
    severity: VulnerabilitySeverity
    confidence: float
    
    # Description and location
    description: str
    location: str
    evidence: str
    
    # Attack and remediation details
    attack_vectors: List[str] = field(default_factory=list)
    remediation: str = ""
    affected_versions: List[str] = field(default_factory=list)
    fixed_version: str = ""
    references: List[str] = field(default_factory=list)
    impact: str = ""
    exploitability: ExploitabilityLevel = ExploitabilityLevel.LOCAL
    
    # Risk assessment
    cvss_score: Optional[float] = None
    risk_score: float = 0.0
    business_impact: str = ""
    
    # Metadata
    discovered_at: datetime = field(default_factory=datetime.now)
    library_type: LibraryType = LibraryType.UTILITY
    detection_method: DetectionMethod = DetectionMethod.GRADLE_ANALYSIS
    
    def __post_init__(self):
        """Validate vulnerability data."""
        if not (0.0 <= self.confidence <= 1.0):
            raise ValueError("Confidence must be between 0.0 and 1.0")
        
        if self.cvss_score is not None and not (0.0 <= self.cvss_score <= 10.0):
            raise ValueError("CVSS score must be between 0.0 and 10.0")
        
        if not (0.0 <= self.risk_score <= 100.0):
            self.risk_score = self._calculate_risk_score()
    
    def _calculate_risk_score(self) -> float:
        """Calculate risk score based on severity and exploitability."""
        try:
            # Base severity scores
            severity_scores = {
                VulnerabilitySeverity.CRITICAL: 90.0,
                VulnerabilitySeverity.HIGH: 70.0,
                VulnerabilitySeverity.MEDIUM: 50.0,
                VulnerabilitySeverity.LOW: 30.0,
                VulnerabilitySeverity.INFO: 10.0
            }
            
            # Exploitability multipliers
            exploitability_multipliers = {
                ExploitabilityLevel.REMOTE: 1.2,
                ExploitabilityLevel.LOCAL: 1.0,
                ExploitabilityLevel.PHYSICAL: 0.7,
                ExploitabilityLevel.DIFFICULT: 0.6,
                ExploitabilityLevel.THEORETICAL: 0.3
            }
            
            base_score = severity_scores.get(self.severity, 50.0)
            multiplier = exploitability_multipliers.get(self.exploitability, 1.0)
            confidence_factor = self.confidence
            
            # Calculate final risk score
            risk_score = base_score * multiplier * confidence_factor
            return min(100.0, max(0.0, risk_score))
            
        except Exception:
            return 50.0  # Default medium risk

@dataclass
class LibraryInfo:
    """Information about a detected library with comprehensive metadata."""
    
    # Core identification
    name: str
    version: str
    location: str
    detection_method: DetectionMethod
    confidence: float
    
    # File and path information
    file_path: str = ""
    checksum: str = ""
    file_size: Optional[int] = None
    
    # Library classification
    library_type: LibraryType = LibraryType.UTILITY
    vendor: str = ""
    license_type: str = ""
    
    # Version information
    latest_version: str = ""
    is_outdated: bool = False
    versions_behind: int = 0
    
    # Security metadata
    known_vulnerabilities: int = 0
    security_score: float = 0.0
    last_updated: Optional[datetime] = None
    
    # Detection details
    detection_patterns: List[str] = field(default_factory=list)
    evidence_sources: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Validate library information."""
        if not (0.0 <= self.confidence <= 1.0):
            raise ValueError("Confidence must be between 0.0 and 1.0")
        
        if not (0.0 <= self.security_score <= 100.0):
            self.security_score = self._calculate_security_score()
    
    def _calculate_security_score(self) -> float:
        """Calculate security score based on library characteristics."""
        try:
            base_score = 80.0  # Start with good score
            
            # Penalize for known vulnerabilities
            if self.known_vulnerabilities > 0:
                penalty = min(self.known_vulnerabilities * 10, 50)
                base_score -= penalty
            
            # Penalize for being outdated
            if self.is_outdated:
                outdated_penalty = min(self.versions_behind * 5, 30)
                base_score -= outdated_penalty
            
            # Adjust based on detection confidence
            base_score *= self.confidence
            
            return max(0.0, min(100.0, base_score))
            
        except Exception:
            return 50.0  # Default medium security score

@dataclass
class VulnerabilityDatabaseEntry:
    """Entry in the vulnerability database with comprehensive CVE information."""
    
    # Core CVE information
    vulnerability_id: str
    cve_id: str
    title: str
    description: str
    severity: VulnerabilitySeverity
    
    # Version information
    affected_versions: List[str]
    fixed_version: str
    
    # Attack information
    attack_vectors: List[str]
    exploitability: ExploitabilityLevel
    impact: str
    
    # Remediation
    remediation: str
    references: List[str]
    
    # Scoring
    cvss_score: Optional[float] = None
    cvss_vector: str = ""
    
    # Metadata
    published_date: Optional[datetime] = None
    last_modified: Optional[datetime] = None
    cwe_ids: List[str] = field(default_factory=list)
    
    # Library-specific information
    applicable_library_types: Set[LibraryType] = field(default_factory=set)
    detection_patterns: List[str] = field(default_factory=list)

@dataclass
class LibraryAnalysisConfig:
    """Configuration for library vulnerability analysis."""
    
    # Analysis scope
    enable_gradle_analysis: bool = True
    enable_binary_analysis: bool = True
    enable_source_analysis: bool = True
    enable_manifest_analysis: bool = True
    
    # Performance settings
    max_analysis_time: int = 300  # 5 minutes
    max_files_to_analyze: int = 1000
    parallel_analysis: bool = True
    max_workers: int = 4
    
    # Detection thresholds
    min_confidence_threshold: float = 0.3
    severity_filter: List[VulnerabilitySeverity] = field(
        default_factory=lambda: list(VulnerabilitySeverity)
    )
    
    # External resources
    vulnerability_db_path: Optional[str] = None
    pattern_config_path: Optional[str] = None
    
    # Reporting
    include_low_confidence: bool = False
    detailed_reporting: bool = True
    include_recommendations: bool = True
    
    def __post_init__(self):
        """Validate configuration."""
        if not (0.0 <= self.min_confidence_threshold <= 1.0):
            raise ValueError("Confidence threshold must be between 0.0 and 1.0")
        
        if self.max_analysis_time <= 0:
            raise ValueError("Max analysis time must be positive")
        
        if self.max_workers <= 0:
            raise ValueError("Max workers must be positive")

@dataclass
class LibraryAnalysisResult:
    """Comprehensive result of library vulnerability analysis."""
    
    # Analysis metadata
    analysis_id: str
    start_time: datetime
    end_time: datetime
    analysis_duration: float
    
    # Analysis configuration
    config: LibraryAnalysisConfig
    
    # Results
    detected_libraries: List[LibraryInfo]
    vulnerabilities: List[LibraryVulnerability]
    
    # Summary statistics
    total_libraries: int = 0
    total_vulnerabilities: int = 0
    critical_vulnerabilities: int = 0
    high_vulnerabilities: int = 0
    medium_vulnerabilities: int = 0
    low_vulnerabilities: int = 0
    
    # Risk assessment
    overall_risk_score: float = 0.0
    risk_level: str = "UNKNOWN"
    
    # Analysis quality
    analysis_coverage: float = 0.0  # Percentage of files analyzed
    confidence_distribution: Dict[str, int] = field(default_factory=dict)
    
    # Errors and warnings
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    # Recommendations
    recommendations: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Calculate derived fields."""
        self.total_libraries = len(self.detected_libraries)
        self.total_vulnerabilities = len(self.vulnerabilities)
        
        # Count vulnerabilities by severity
        for vuln in self.vulnerabilities:
            if vuln.severity == VulnerabilitySeverity.CRITICAL:
                self.critical_vulnerabilities += 1
            elif vuln.severity == VulnerabilitySeverity.HIGH:
                self.high_vulnerabilities += 1
            elif vuln.severity == VulnerabilitySeverity.MEDIUM:
                self.medium_vulnerabilities += 1
            elif vuln.severity == VulnerabilitySeverity.LOW:
                self.low_vulnerabilities += 1
        
        # Calculate overall risk score
        if not self.overall_risk_score:
            self.overall_risk_score = self._calculate_overall_risk()
        
        # Determine risk level
        if not self.risk_level or self.risk_level == "UNKNOWN":
            self.risk_level = self._determine_risk_level()
    
    def _calculate_overall_risk(self) -> float:
        """Calculate overall risk score based on all vulnerabilities."""
        if not self.vulnerabilities:
            return 0.0
        
        try:
            total_weighted_score = 0.0
            total_weight = 0.0
            
            for vuln in self.vulnerabilities:
                weight = vuln.confidence
                total_weighted_score += vuln.risk_score * weight
                total_weight += weight
            
            if total_weight > 0:
                return total_weighted_score / total_weight
            else:
                return 0.0
                
        except Exception:
            return 0.0
    
    def _determine_risk_level(self) -> str:
        """Determine risk level based on overall risk score."""
        if self.overall_risk_score >= 80:
            return "CRITICAL"
        elif self.overall_risk_score >= 60:
            return "HIGH"
        elif self.overall_risk_score >= 40:
            return "MEDIUM"
        elif self.overall_risk_score >= 20:
            return "LOW"
        else:
            return "MINIMAL"

@dataclass
class ConfidenceEvidence:
    """Evidence structure for confidence calculation."""
    
    # Detection evidence
    detection_method: DetectionMethod
    pattern_matches: int
    pattern_quality: float
    version_certainty: float
    
    # Context evidence
    file_context: str
    location_relevance: float
    cross_validation: bool
    
    # Library evidence
    library_popularity: float
    vendor_reputation: float
    update_frequency: float
    
    # Vulnerability evidence
    cve_validation: bool
    exploit_availability: bool
    patch_availability: bool
    
    # Analysis metadata
    analysis_depth: str = "standard"  # basic, standard, deep
    validation_sources: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Validate evidence data."""
        float_fields = [
            'pattern_quality', 'version_certainty', 'location_relevance',
            'library_popularity', 'vendor_reputation', 'update_frequency'
        ]
        
        for field_name in float_fields:
            value = getattr(self, field_name)
            if not (0.0 <= value <= 1.0):
                raise ValueError(f"{field_name} must be between 0.0 and 1.0") 