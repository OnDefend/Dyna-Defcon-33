#!/usr/bin/env python3
"""
Reusable Class Analysis Helper for Advanced Vulnerability Detection

This module provides a unified, parallelized approach to class analysis,
eliminating code duplication and improving performance.
"""

import re
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Callable, Any, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path

from .data_structures import (
    VulnerabilityMatch,
    VulnerabilityFinding,
    CategoryAnalysisResult,
    VulnerabilityCategory,
    SeverityLevel,
    CompiledPattern,
    AnalysisConfiguration
)

logger = logging.getLogger(__name__)

@dataclass
class ClassAnalysisContext:
    """Context information for class analysis."""
    class_name: str
    class_source: str
    class_type: str
    file_path: str = ""
    metadata: Dict[str, Any] = None
    
    def __post_init__(self) -> None:
        if self.metadata is None:
            self.metadata = {}

class UniversalClassAnalyzer:
    """
    Universal class analyzer that provides reusable analysis capabilities
    with parallelization support and optimized pattern matching.
    """
    
    def __init__(self, apk_ctx, config: AnalysisConfiguration):
        """
        Initialize the universal class analyzer.
        
        Args:
            apk_ctx: APK context object
            config: Analysis configuration
        """
        self.apk_ctx = apk_ctx
        self.config = config
        self.compiled_patterns: Dict[str, List[CompiledPattern]] = {}
        self.comprehensive_analyzer = None
        
        # Lazy-load comprehensive analyzer with fail-fast
        self._load_comprehensive_analyzer()
        
        # Pre-validate required APK context attributes
        self._validate_apk_context()
    
    def _load_comprehensive_analyzer(self) -> None:
        """Lazy-load comprehensive analyzer with fail-fast error handling."""
        try:
            from core.comprehensive_class_analyzer import create_comprehensive_analyzer
            self.comprehensive_analyzer = create_comprehensive_analyzer
        except ImportError as e:
            logger.error(f"Failed to load comprehensive class analyzer: {e}")
            raise RuntimeError(f"Comprehensive class analyzer not available: {e}")
    
    def _validate_apk_context(self) -> None:
        """Validate required APK context attributes early."""
        required_attrs = ['package_name', 'classes']
        missing_attrs = []
        
        for attr in required_attrs:
            if not hasattr(self.apk_ctx, attr) or getattr(self.apk_ctx, attr) is None:
                missing_attrs.append(attr)
        
        if missing_attrs:
            raise ValueError(f"APK context missing required attributes: {missing_attrs}")
    
    def compile_patterns(self, patterns: Dict[str, Dict[str, List[str]]], 
                        category: VulnerabilityCategory) -> None:
        """
        Pre-compile regex patterns for efficient matching.
        
        Args:
            patterns: Pattern dictionary from configuration
            category: Vulnerability category
        """
        category_key = category.value
        self.compiled_patterns[category_key] = []
        
        for subcategory, pattern_list in patterns.items():
            for pattern_str in pattern_list:
                try:
                    compiled_regex = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                    compiled_pattern = CompiledPattern(
                        pattern=pattern_str,
                        compiled_regex=compiled_regex,
                        category=category_key,
                        subcategory=subcategory,
                        severity=self._get_pattern_severity(subcategory),
                        confidence_weight=self._get_pattern_confidence_weight(subcategory)
                    )
                    self.compiled_patterns[category_key].append(compiled_pattern)
                except re.error as e:
                    logger.warning(f"Invalid regex pattern '{pattern_str}': {e}")
                    continue
    
    def _get_pattern_severity(self, subcategory: str) -> SeverityLevel:
        """Get severity level for a pattern subcategory."""
        severity_map = {
            # SQL injection severities
            'content_provider_queries': SeverityLevel.HIGH,
            'dynamic_sql_construction': SeverityLevel.CRITICAL,
            'unsafe_sql_methods': SeverityLevel.HIGH,
            
            # Path traversal severities
            'file_operations': SeverityLevel.MEDIUM,
            'path_manipulation': SeverityLevel.HIGH,
            'unsafe_path_methods': SeverityLevel.MEDIUM,
            
            # Data storage severities
            'shared_preferences': SeverityLevel.MEDIUM,
            'internal_storage': SeverityLevel.LOW,
            'external_storage': SeverityLevel.HIGH,
            'database_operations': SeverityLevel.MEDIUM,
            
            # Injection attack severities
            'xpath_injection': SeverityLevel.HIGH,
            'ldap_injection': SeverityLevel.HIGH,
            'command_injection': SeverityLevel.CRITICAL,
            'script_injection': SeverityLevel.HIGH,
            
            # Sensitive data severities
            'credentials': SeverityLevel.CRITICAL,
            'personal_info': SeverityLevel.HIGH,
        }
        return severity_map.get(subcategory, SeverityLevel.MEDIUM)
    
    def _get_pattern_confidence_weight(self, subcategory: str) -> float:
        """Get confidence weight for a pattern subcategory."""
        confidence_map = {
            # High confidence patterns
            'dynamic_sql_construction': 0.9,
            'path_manipulation': 0.95,
            'command_injection': 0.9,
            'credentials': 0.85,
            
            # Medium confidence patterns
            'content_provider_queries': 0.7,
            'file_operations': 0.6,
            'unsafe_sql_methods': 0.75,
            'xpath_injection': 0.7,
            'ldap_injection': 0.7,
            'script_injection': 0.75,
            'personal_info': 0.7,
            
            # Lower confidence patterns
            'shared_preferences': 0.5,
            'internal_storage': 0.4,
            'external_storage': 0.6,
            'database_operations': 0.5,
            'unsafe_path_methods': 0.4,
        }
        return confidence_map.get(subcategory, 0.6)
    
    def analyze_category_parallel(self, 
                                 category: VulnerabilityCategory,
                                 analysis_callback: Callable[[ClassAnalysisContext, List[CompiledPattern]], List[VulnerabilityFinding]],
                                 progress_desc: str = "Analyzing") -> CategoryAnalysisResult:
        """
        Analyze a vulnerability category using parallel processing.
        
        Args:
            category: Vulnerability category to analyze
            analysis_callback: Function to analyze individual classes
            progress_desc: Description for progress logging
            
        Returns:
            Category analysis results
        """
        logger.info(f"Starting {progress_desc.lower()} for {category.value}")
        
        category_key = category.value
        if category_key not in self.compiled_patterns:
            logger.warning(f"No compiled patterns for category {category_key}")
            return CategoryAnalysisResult(category=category)
        
        patterns = self.compiled_patterns[category_key]
        result = CategoryAnalysisResult(category=category)
        
        try:
            # Create comprehensive analyzer
            analyzer = self.comprehensive_analyzer(self.apk_ctx, category_key)
            
            # Collect all classes for analysis
            class_contexts = self._prepare_class_contexts()
            result.classes_analyzed = len(class_contexts)
            
            # Analyze classes in parallel
            findings = self._analyze_classes_parallel(class_contexts, patterns, analysis_callback)
            
            # Limit findings to avoid overwhelming output
            if len(findings) > self.config.max_findings_per_category:
                logger.info(f"Limiting {category.value} findings to {self.config.max_findings_per_category}")
                findings = findings[:self.config.max_findings_per_category]
            
            result.findings = findings
            result.risk_score = self._calculate_category_risk_score(findings)
            
            logger.info(f"Completed {progress_desc.lower()}: {len(findings)} findings, "
                       f"risk score: {result.risk_score:.1f}")
            
        except Exception as e:
            logger.error(f"Error in {progress_desc.lower()}: {e}", 
                        extra={'category': category.value, 'error': str(e)})
            raise
        
        return result
    
    def _prepare_class_contexts(self) -> List[ClassAnalysisContext]:
        """Prepare class contexts for analysis."""
        contexts = []
        
        # Safely iterate through classes
        try:
            if hasattr(self.apk_ctx, 'classes') and self.apk_ctx.classes:
                for class_item in self.apk_ctx.classes:
                    try:
                        # Extract class information safely
                        class_info = self._extract_class_info(class_item)
                        if class_info and class_info.get('type') != 'string':
                            context = ClassAnalysisContext(
                                class_name=class_info['name'],
                                class_source=class_info.get('source', ''),
                                class_type=class_info.get('type', 'unknown'),
                                file_path=class_info.get('file_path', ''),
                                metadata=class_info
                            )
                            contexts.append(context)
                    except Exception as e:
                        logger.debug(f"Error preparing class context: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error accessing APK classes: {e}")
            raise
        
        return contexts
    
    def _extract_class_info(self, class_item) -> Optional[Dict[str, Any]]:
        """Extract class information safely."""
        try:
            # Determine class name
            if hasattr(class_item, 'name'):
                class_name = class_item.name
            elif hasattr(class_item, 'get_name'):
                class_name = class_item.get_name()
            else:
                class_name = str(class_item)
            
            # Determine class type
            class_type = getattr(class_item, 'type', 'class')
            
            # Extract source code
            class_source = ""
            if hasattr(class_item, 'get_source'):
                try:
                    class_source = class_item.get_source()
                except Exception:
                    class_source = str(class_item)
            else:
                class_source = str(class_item)
            
            return {
                'name': class_name,
                'type': class_type,
                'source': class_source,
                'file_path': getattr(class_item, 'file_path', ''),
                'raw_item': class_item
            }
            
        except Exception as e:
            logger.debug(f"Error extracting class info: {e}")
            return None
    
    def _analyze_classes_parallel(self, 
                                 contexts: List[ClassAnalysisContext],
                                 patterns: List[CompiledPattern],
                                 analysis_callback: Callable) -> List[VulnerabilityFinding]:
        """Analyze classes in parallel using unified performance optimization framework."""
        findings = []
        
        # Use unified parallel processing if multiple workers configured
        if self.config.parallel_workers > 1 and len(contexts) > 1:
            try:
                # Import and use unified performance optimization framework
                from core.performance_optimizer import ParallelProcessor
                from functools import partial
                
                # Create parallel processor with unified framework
                parallel_processor = ParallelProcessor(max_workers=self.config.parallel_workers)
                
                # Create a partial function that includes patterns for each context
                analyze_func = partial(self._analyze_context_with_patterns, 
                                     patterns=patterns, 
                                     analysis_callback=analysis_callback)
                
                # Process contexts using unified parallel framework
                results = parallel_processor.process_parallel(
                    items=contexts,
                    processor_func=analyze_func,
                    timeout=30  # 30 seconds per context
                )
                
                # Flatten results and filter out None values
                for context_findings in results:
                    if context_findings:
                        findings.extend(context_findings)
                
                logger.info(f"Unified parallel analysis completed: {len(contexts)} contexts, "
                           f"{len(findings)} findings")
                
            except Exception as e:
                logger.warning(f"Unified performance framework failed, using fallback: {e}")
                # Fallback to original ThreadPoolExecutor implementation
                findings = self._analyze_classes_parallel_fallback(contexts, patterns, analysis_callback)
        else:
            # Sequential processing for small workloads
            findings = self._analyze_classes_sequential(contexts, patterns, analysis_callback)
        
        return findings
    
    def _analyze_context_with_patterns(self, context: ClassAnalysisContext, 
                                     patterns: List[CompiledPattern],
                                     analysis_callback: Callable) -> List[VulnerabilityFinding]:
        """Wrapper method for unified parallel processing compatibility."""
        try:
            return analysis_callback(context, patterns)
        except Exception as e:
            logger.debug(f"Error analyzing class {context.class_name}: {e}",
                       extra={'class_name': context.class_name, 'error': str(e)})
            return []
    
    def _analyze_classes_parallel_fallback(self, 
                                         contexts: List[ClassAnalysisContext],
                                         patterns: List[CompiledPattern],
                                         analysis_callback: Callable) -> List[VulnerabilityFinding]:
        """Fallback parallel processing method using ThreadPoolExecutor."""
        findings = []
        
        try:
            with ThreadPoolExecutor(max_workers=self.config.parallel_workers) as executor:
                # Submit analysis tasks
                future_to_context = {
                    executor.submit(analysis_callback, context, patterns): context 
                    for context in contexts
                }
                
                # Collect results as they complete
                for future in as_completed(future_to_context):
                    context = future_to_context[future]
                    try:
                        context_findings = future.result()
                        if context_findings:
                            findings.extend(context_findings)
                    except Exception as e:
                        logger.debug(f"Error analyzing class {context.class_name}: {e}",
                                   extra={'class_name': context.class_name, 'error': str(e)})
        except Exception as e:
            logger.warning(f"Parallel analysis failed, falling back to sequential: {e}")
            # Final fallback to sequential processing
            findings = self._analyze_classes_sequential(contexts, patterns, analysis_callback)
        
        return findings
    
    def _analyze_classes_sequential(self, 
                                   contexts: List[ClassAnalysisContext],
                                   patterns: List[CompiledPattern],
                                   analysis_callback: Callable) -> List[VulnerabilityFinding]:
        """Analyze classes sequentially."""
        findings = []
        
        for context in contexts:
            try:
                context_findings = analysis_callback(context, patterns)
                if context_findings:
                    findings.extend(context_findings)
            except Exception as e:
                logger.debug(f"Error analyzing class {context.class_name}: {e}",
                           extra={'class_name': context.class_name, 'error': str(e)})
        
        return findings
    
    def _calculate_category_risk_score(self, findings: List[VulnerabilityFinding]) -> float:
        """Calculate risk score for a category based on findings."""
        if not findings:
            return 0.0
        
        total_score = 0.0
        total_weight = 0.0
        
        severity_weights = {
            SeverityLevel.LOW: 1.0,
            SeverityLevel.MEDIUM: 2.5,
            SeverityLevel.HIGH: 5.0,
            SeverityLevel.CRITICAL: 10.0
        }
        
        for finding in findings:
            severity_weight = severity_weights.get(finding.max_severity, 1.0)
            confidence_weight = finding.confidence
            match_count_weight = min(finding.total_matches / 10.0, 1.0)  # Cap at 1.0
            
            score = severity_weight * confidence_weight * (1.0 + match_count_weight)
            total_score += score
            total_weight += severity_weight
        
        # Normalize score
        if total_weight > 0:
            return min((total_score / total_weight) * 10.0, 100.0)  # Scale to 0-100
        return 0.0

def create_pattern_analyzer(context: ClassAnalysisContext, 
                           patterns: List[CompiledPattern]) -> List[VulnerabilityFinding]:
    """
    Generic pattern analysis function that can be used as a callback.
    
    Args:
        context: Class analysis context
        patterns: Compiled patterns to match
        
    Returns:
        List of vulnerability findings
    """
    findings = []
    
    if not context.class_source:
        return findings
    
    # Group patterns by subcategory for better organization
    pattern_groups = {}
    for pattern in patterns:
        if pattern.subcategory not in pattern_groups:
            pattern_groups[pattern.subcategory] = []
        pattern_groups[pattern.subcategory].append(pattern)
    
    # Analyze each pattern group
    for subcategory, group_patterns in pattern_groups.items():
        matches = []
        
        for compiled_pattern in group_patterns:
            try:
                regex_matches = list(compiled_pattern.compiled_regex.finditer(context.class_source))
                
                for match in regex_matches:
                    vuln_match = VulnerabilityMatch(
                        pattern=compiled_pattern.pattern,
                        match_text=match.group(0),
                        line_number=context.class_source[:match.start()].count('\n') + 1,
                        column_start=match.start(),
                        column_end=match.end(),
                        context=_extract_match_context(context.class_source, match),
                        severity=compiled_pattern.severity,
                        confidence=compiled_pattern.confidence_weight
                    )
                    matches.append(vuln_match)
                    
            except Exception as e:
                logger.debug(f"Pattern matching error for {compiled_pattern.pattern}: {e}")
                continue
        
        # Create finding if matches found
        if matches:
            finding = VulnerabilityFinding(
                class_name=context.class_name,
                vulnerability_type=subcategory,
                category=VulnerabilityCategory(patterns[0].category),
                matches=matches
            )
            findings.append(finding)
    
    return findings

def _extract_match_context(source: str, match: re.Match, context_lines: int = 2) -> str:
    """Extract context around a regex match."""
    try:
        lines = source.split('\n')
        match_line = source[:match.start()].count('\n')
        
        start_line = max(0, match_line - context_lines)
        end_line = min(len(lines), match_line + context_lines + 1)
        
        context_lines_list = lines[start_line:end_line]
        return '\n'.join(context_lines_list)
    except Exception:
        return match.group(0)  # Fallback to just the match 