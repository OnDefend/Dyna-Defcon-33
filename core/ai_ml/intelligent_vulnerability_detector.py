#!/usr/bin/env python3
"""
Intelligent Vulnerability Detection Engine

AI/ML-powered vulnerability detection that enhances traditional pattern-based
detection with machine learning, contextual analysis, and adaptive learning.
"""

import logging
import json
import hashlib
import pickle
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import re

# ML imports with fallback
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.ensemble import RandomForestClassifier, IsolationForest
    from sklearn.cluster import DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import cross_val_score
    import pandas as pd
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class VulnerabilityPattern:
    """Enhanced vulnerability pattern with ML features."""
    pattern_id: str
    pattern_text: str
    vulnerability_type: str
    severity: str
    confidence: float
    frequency: int = 0
    accuracy_score: float = 0.0
    false_positive_rate: float = 0.0
    last_updated: datetime = None
    feature_vector: Optional[List[float]] = None
    context_indicators: List[str] = None
    
    def __post_init__(self):
        if self.last_updated is None:
            self.last_updated = datetime.now()
        if self.context_indicators is None:
            self.context_indicators = []

@dataclass
class DetectionResult:
    """AI-enhanced detection result."""
    is_vulnerability: bool
    vulnerability_type: str
    severity: str
    confidence: float
    ml_confidence: float
    pattern_matches: List[str]
    ml_features: Dict[str, float]
    contextual_evidence: List[str]
    false_positive_probability: float
    recommendation: str
    explanation: str

class IntelligentVulnerabilityDetector:
    """
    AI/ML-powered vulnerability detection engine.
    
    Combines traditional pattern matching with machine learning for:
    - Enhanced accuracy through context analysis
    - Reduced false positives via ML filtering
    - Adaptive learning from historical data
    - Intelligent pattern evolution
    """
    
    def __init__(self, model_cache_dir: str = "models/vulnerability_detection"):
        """Initialize the intelligent vulnerability detector."""
        self.logger = logging.getLogger(__name__)
        self.model_cache_dir = Path(model_cache_dir)
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Core components
        self.vulnerability_patterns = {}
        self.ml_models = {}
        self.feature_extractors = {}
        self.context_analyzers = {}
        
        # Learning components
        self.training_data = []
        self.feedback_data = []
        self.performance_metrics = defaultdict(list)
        
        # Initialize components
        self._initialize_traditional_patterns()
        if ML_AVAILABLE:
            self._initialize_ml_components()
            self._load_or_create_models()
        else:
            self.logger.warning("ML libraries not available - using pattern-based detection only")
        
        self.logger.info("Intelligent Vulnerability Detector initialized")
    
    def _initialize_traditional_patterns(self):
        """Initialize traditional vulnerability patterns with AI enhancements."""
        
        # Enhanced patterns with contextual indicators
        enhanced_patterns = {
            "sql_injection": VulnerabilityPattern(
                pattern_id="sql_injection_enhanced",
                pattern_text=r"(?i)(?:sql.*injection|database.*injection|sqli)",
                vulnerability_type="SQL_INJECTION",
                severity="HIGH",
                confidence=0.85,
                context_indicators=["database", "query", "select", "insert", "update", "delete"]
            ),
            
            "hardcoded_secrets": VulnerabilityPattern(
                pattern_id="hardcoded_secrets_enhanced", 
                pattern_text=r"(?i)(?:hardcoded.*(?:password|secret|key|token|credential))",
                vulnerability_type="HARDCODED_SECRETS",
                severity="HIGH",
                confidence=0.90,
                context_indicators=["api", "password", "secret", "key", "token", "credential"]
            ),
            
            "exported_components": VulnerabilityPattern(
                pattern_id="exported_components_enhanced",
                pattern_text=r"(?i)(?:exported.*(?:activity|service|receiver|provider)).*?(?:without.*permission|permission.*null)",
                vulnerability_type="EXPORTED_COMPONENTS",
                severity="MEDIUM",
                confidence=0.80,
                context_indicators=["android", "manifest", "exported", "permission"]
            ),
            
            "weak_crypto": VulnerabilityPattern(
                pattern_id="weak_crypto_enhanced",
                pattern_text=r"(?i)(?:weak.*(?:encryption|crypto)|insecure.*(?:random|hash)|md5|sha1|des)",
                vulnerability_type="WEAK_CRYPTOGRAPHY",
                severity="MEDIUM",
                confidence=0.75,
                context_indicators=["encryption", "crypto", "hash", "cipher", "algorithm"]
            ),
            
            "xss_vulnerability": VulnerabilityPattern(
                pattern_id="xss_enhanced",
                pattern_text=r"(?i)(?:xss|cross.*site.*scripting|script.*injection)",
                vulnerability_type="XSS",
                severity="MEDIUM",
                confidence=0.80,
                context_indicators=["javascript", "script", "html", "webview", "input"]
            ),
            
            "insecure_storage": VulnerabilityPattern(
                pattern_id="insecure_storage_enhanced",
                pattern_text=r"(?i)(?:insecure.*storage|cleartext.*storage|unencrypted.*data)",
                vulnerability_type="INSECURE_STORAGE",
                severity="MEDIUM",
                confidence=0.75,
                context_indicators=["storage", "file", "database", "cache", "cleartext"]
            )
        }
        
        self.vulnerability_patterns.update(enhanced_patterns)
        self.logger.info(f"Initialized {len(enhanced_patterns)} enhanced vulnerability patterns")
    
    def _initialize_ml_components(self):
        """Initialize machine learning components."""
        if not ML_AVAILABLE:
            return
        
        # Feature extractors
        self.feature_extractors = {
            "text_features": TfidfVectorizer(
                max_features=1000,
                ngram_range=(1, 3),
                stop_words='english',
                lowercase=True
            ),
            "contextual_features": TfidfVectorizer(
                max_features=500,
                ngram_range=(1, 2),
                analyzer='word'
            )
        }
        
        # ML models
        self.ml_models = {
            "vulnerability_classifier": RandomForestClassifier(
                n_estimators=100,
                max_depth=20,
                random_state=42,
                class_weight='balanced'
            ),
            "false_positive_detector": IsolationForest(
                contamination=0.1,
                random_state=42
            ),
            "severity_predictor": RandomForestClassifier(
                n_estimators=50,
                max_depth=15,
                random_state=42
            ),
            "pattern_clusterer": DBSCAN(
                eps=0.3,
                min_samples=2
            )
        }
        
        # Context analyzers
        self.context_analyzers = {
            "code_context": self._analyze_code_context,
            "file_context": self._analyze_file_context,
            "semantic_context": self._analyze_semantic_context
        }
        
        self.scaler = StandardScaler()
        self.logger.info("ML components initialized")
    
    def detect_vulnerabilities(self, content: str, title: str = "", 
                             file_path: str = "", context: Dict[str, Any] = None) -> DetectionResult:
        """
        Detect vulnerabilities using AI/ML-enhanced analysis.
        
        Args:
            content: Text content to analyze
            title: Title or summary of the content
            file_path: Path of the file being analyzed
            context: Additional context information
            
        Returns:
            DetectionResult with AI-enhanced analysis
        """
        full_text = f"{title} {content}".strip()
        context = context or {}
        
        # Traditional pattern matching
        pattern_results = self._traditional_pattern_detection(full_text)
        
        # ML-enhanced analysis
        ml_results = {}
        if ML_AVAILABLE and self.ml_models:
            ml_results = self._ml_enhanced_detection(full_text, file_path, context)
        
        # Contextual analysis
        contextual_analysis = self._contextual_analysis(full_text, file_path, context)
        
        # Combine results
        final_result = self._combine_detection_results(
            pattern_results, ml_results, contextual_analysis, full_text
        )
        
        # Learn from detection
        self._record_detection_for_learning(full_text, final_result, context)
        
        return final_result
    
    def _traditional_pattern_detection(self, text: str) -> Dict[str, Any]:
        """Traditional pattern-based vulnerability detection."""
        matches = []
        max_confidence = 0.0
        best_match = None
        
        for pattern_id, pattern in self.vulnerability_patterns.items():
            if re.search(pattern.pattern_text, text, re.IGNORECASE | re.MULTILINE):
                # Calculate enhanced confidence with context
                context_score = self._calculate_context_score(text, pattern.context_indicators)
                enhanced_confidence = min(pattern.confidence + (context_score * 0.2), 1.0)
                
                match_info = {
                    "pattern_id": pattern_id,
                    "pattern": pattern,
                    "confidence": enhanced_confidence,
                    "context_score": context_score
                }
                matches.append(match_info)
                
                if enhanced_confidence > max_confidence:
                    max_confidence = enhanced_confidence
                    best_match = match_info
        
        return {
            "matches": matches,
            "best_match": best_match,
            "has_vulnerability": len(matches) > 0,
            "max_confidence": max_confidence
        }
    
    def _ml_enhanced_detection(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ML-enhanced vulnerability detection."""
        try:
            # Extract features
            features = self._extract_ml_features(text, file_path, context)
            
            # Vulnerability classification
            vuln_prediction = self._predict_vulnerability(features)
            
            # False positive detection
            fp_score = self._predict_false_positive(features)
            
            # Severity prediction
            severity_prediction = self._predict_severity(features)
            
            return {
                "ml_confidence": vuln_prediction.get("confidence", 0.0),
                "is_vulnerability": vuln_prediction.get("is_vulnerability", False),
                "predicted_type": vuln_prediction.get("type", "UNKNOWN"),
                "false_positive_score": fp_score,
                "predicted_severity": severity_prediction,
                "features": features
            }
            
        except Exception as e:
            self.logger.error(f"ML detection failed: {e}")
            return {
                "ml_confidence": 0.0,
                "is_vulnerability": False,
                "error": str(e)
            }
    
    def _extract_ml_features(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, float]:
        """Extract ML features from text and context."""
        features = {}
        
        # Text-based features
        features["text_length"] = len(text)
        features["word_count"] = len(text.split())
        features["line_count"] = text.count('\n') + 1
        
        # Vulnerability indicator features
        vuln_keywords = ["vulnerability", "security", "exploit", "attack", "breach", "flaw"]
        features["vuln_keyword_density"] = sum(1 for word in vuln_keywords if word in text.lower()) / len(text.split())
        
        # Technical indicator features
        tech_keywords = ["android", "java", "crypto", "sql", "http", "ssl", "api"]
        features["tech_keyword_density"] = sum(1 for word in tech_keywords if word in text.lower()) / len(text.split())
        
        # Severity indicator features
        severity_keywords = ["critical", "high", "severe", "dangerous", "urgent"]
        features["severity_keyword_count"] = sum(1 for word in severity_keywords if word in text.lower())
        
        # Code-specific features
        features["has_code_patterns"] = 1.0 if re.search(r'[{}();]', text) else 0.0
        features["has_file_paths"] = 1.0 if re.search(r'[/\\]\w+[/\\]', text) else 0.0
        features["has_urls"] = 1.0 if re.search(r'https?://', text) else 0.0
        
        # File context features
        if file_path:
            features["is_manifest"] = 1.0 if "manifest" in file_path.lower() else 0.0
            features["is_source_code"] = 1.0 if any(ext in file_path.lower() for ext in ['.java', '.kt', '.xml']) else 0.0
            features["is_config"] = 1.0 if any(ext in file_path.lower() for ext in ['.properties', '.config', '.xml']) else 0.0
        
        # Context features
        if context:
            features["plugin_confidence"] = context.get("confidence", 0.0)
            features["plugin_type_score"] = self._get_plugin_type_score(context.get("plugin_type", ""))
        
        return features
    
    def _contextual_analysis(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Perform contextual analysis of the detection."""
        analysis = {
            "context_score": 0.0,
            "context_evidence": [],
            "risk_factors": [],
            "mitigating_factors": []
        }
        
        # Analyze each context type
        for analyzer_name, analyzer_func in self.context_analyzers.items():
            try:
                result = analyzer_func(text, file_path, context)
                analysis[analyzer_name] = result
                analysis["context_score"] += result.get("score", 0.0)
                analysis["context_evidence"].extend(result.get("evidence", []))
            except Exception as e:
                self.logger.debug(f"Context analyzer {analyzer_name} failed: {e}")
        
        # Normalize context score
        analysis["context_score"] = min(analysis["context_score"] / len(self.context_analyzers), 1.0)
        
        return analysis
    
    def _analyze_code_context(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze code-specific context."""
        score = 0.0
        evidence = []
        
        # Check for code patterns that increase confidence
        if re.search(r'(class|function|method|import)', text, re.IGNORECASE):
            score += 0.3
            evidence.append("contains_code_structure")
        
        if re.search(r'(try|catch|throw|exception)', text, re.IGNORECASE):
            score += 0.2
            evidence.append("contains_error_handling")
        
        if re.search(r'(if|else|while|for|switch)', text, re.IGNORECASE):
            score += 0.2
            evidence.append("contains_control_flow")
        
        return {"score": score, "evidence": evidence}
    
    def _analyze_file_context(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze file-specific context."""
        score = 0.0
        evidence = []
        
        if not file_path:
            return {"score": 0.0, "evidence": ["no_file_path"]}
        
        file_path_lower = file_path.lower()
        
        # High-risk file types
        if any(ext in file_path_lower for ext in ['.java', '.kt', '.xml']):
            score += 0.4
            evidence.append("high_risk_file_type")
        
        # Configuration files
        if any(name in file_path_lower for name in ['manifest', 'config', 'properties']):
            score += 0.3
            evidence.append("configuration_file")
        
        # Security-related files
        if any(name in file_path_lower for name in ['security', 'auth', 'crypto', 'ssl']):
            score += 0.5
            evidence.append("security_related_file")
        
        return {"score": score, "evidence": evidence}
    
    def _analyze_semantic_context(self, text: str, file_path: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze semantic context of the vulnerability."""
        score = 0.0
        evidence = []
        
        # Security domain analysis
        security_domains = {
            "authentication": ["auth", "login", "password", "credential"],
            "authorization": ["permission", "access", "privilege", "role"],
            "cryptography": ["crypto", "encrypt", "cipher", "hash", "ssl", "tls"],
            "data_protection": ["storage", "database", "file", "cache", "privacy"],
            "network_security": ["http", "network", "communication", "protocol"]
        }
        
        text_lower = text.lower()
        for domain, keywords in security_domains.items():
            if sum(1 for keyword in keywords if keyword in text_lower) >= 2:
                score += 0.2
                evidence.append(f"security_domain_{domain}")
        
        # Vulnerability language analysis
        vuln_language = ["vulnerable", "insecure", "weak", "exposed", "flaw", "issue"]
        vuln_count = sum(1 for word in vuln_language if word in text_lower)
        if vuln_count >= 2:
            score += 0.3
            evidence.append(f"vulnerability_language_count_{vuln_count}")
        
        return {"score": score, "evidence": evidence}
    
    def _combine_detection_results(self, pattern_results: Dict[str, Any], 
                                 ml_results: Dict[str, Any], 
                                 contextual_analysis: Dict[str, Any],
                                 text: str) -> DetectionResult:
        """Combine all detection results into final result."""
        
        # Determine if it's a vulnerability
        pattern_vuln = pattern_results.get("has_vulnerability", False)
        ml_vuln = ml_results.get("is_vulnerability", False)
        
        # Combined confidence calculation
        pattern_confidence = pattern_results.get("max_confidence", 0.0)
        ml_confidence = ml_results.get("ml_confidence", 0.0)
        context_score = contextual_analysis.get("context_score", 0.0)
        
        # Weighted combination
        combined_confidence = (
            pattern_confidence * 0.4 + 
            ml_confidence * 0.4 + 
            context_score * 0.2
        )
        
        # False positive probability
        fp_probability = ml_results.get("false_positive_score", 0.0)
        
        # Determine final classification
        is_vulnerability = (pattern_vuln or ml_vuln) and combined_confidence > 0.5 and fp_probability < 0.7
        
        # Determine vulnerability type and severity
        if pattern_results.get("best_match"):
            vuln_type = pattern_results["best_match"]["pattern"].vulnerability_type
            severity = pattern_results["best_match"]["pattern"].severity
        else:
            vuln_type = ml_results.get("predicted_type", "UNKNOWN")
            severity = ml_results.get("predicted_severity", "MEDIUM")
        
        # Generate explanation
        explanation = self._generate_explanation(
            pattern_results, ml_results, contextual_analysis, combined_confidence
        )
        
        # Generate recommendation
        recommendation = self._generate_recommendation(is_vulnerability, vuln_type, severity, fp_probability)
        
        return DetectionResult(
            is_vulnerability=is_vulnerability,
            vulnerability_type=vuln_type,
            severity=severity,
            confidence=combined_confidence,
            ml_confidence=ml_confidence,
            pattern_matches=[m["pattern_id"] for m in pattern_results.get("matches", [])],
            ml_features=ml_results.get("features", {}),
            contextual_evidence=contextual_analysis.get("context_evidence", []),
            false_positive_probability=fp_probability,
            recommendation=recommendation,
            explanation=explanation
        )
    
    def _generate_explanation(self, pattern_results: Dict[str, Any], 
                            ml_results: Dict[str, Any], 
                            contextual_analysis: Dict[str, Any],
                            confidence: float) -> str:
        """Generate human-readable explanation of the detection."""
        explanations = []
        
        # Pattern explanation
        if pattern_results.get("matches"):
            match_count = len(pattern_results["matches"])
            explanations.append(f"Matched {match_count} vulnerability pattern(s)")
        
        # ML explanation
        if ml_results.get("ml_confidence", 0) > 0.5:
            explanations.append(f"ML analysis indicates {ml_results['ml_confidence']:.1%} vulnerability probability")
        
        # Context explanation
        if contextual_analysis.get("context_score", 0) > 0.3:
            explanations.append("Strong contextual indicators support vulnerability classification")
        
        # Confidence explanation
        if confidence > 0.8:
            explanations.append("High confidence in detection accuracy")
        elif confidence > 0.6:
            explanations.append("Moderate confidence in detection")
        else:
            explanations.append("Low confidence - requires manual review")
        
        return ". ".join(explanations) + "."
    
    def _generate_recommendation(self, is_vulnerability: bool, vuln_type: str, 
                               severity: str, fp_probability: float) -> str:
        """Generate actionable recommendation."""
        if not is_vulnerability:
            if fp_probability > 0.7:
                return "Likely false positive - safe to ignore"
            else:
                return "No vulnerability detected - continue monitoring"
        
        if fp_probability > 0.5:
            return f"Potential {severity.lower()} severity {vuln_type} vulnerability detected, but high false positive probability - manual review recommended"
        
        if severity == "HIGH" or severity == "CRITICAL":
            return f"Critical {vuln_type} vulnerability detected - immediate remediation required"
        elif severity == "MEDIUM":
            return f"Medium severity {vuln_type} vulnerability detected - plan remediation"
        else:
            return f"Low severity {vuln_type} vulnerability detected - monitor and address when convenient"
    
    def _calculate_context_score(self, text: str, context_indicators: List[str]) -> float:
        """Calculate context relevance score."""
        if not context_indicators:
            return 0.0
        
        text_lower = text.lower()
        matches = sum(1 for indicator in context_indicators if indicator in text_lower)
        return min(matches / len(context_indicators), 1.0)
    
    def _get_plugin_type_score(self, plugin_type: str) -> float:
        """Get relevance score for plugin type."""
        high_value_types = ["security", "vulnerability", "crypto", "auth"]
        medium_value_types = ["manifest", "code", "static"]
        
        plugin_type_lower = plugin_type.lower()
        
        if any(t in plugin_type_lower for t in high_value_types):
            return 1.0
        elif any(t in plugin_type_lower for t in medium_value_types):
            return 0.7
        else:
            return 0.3
    
    def _record_detection_for_learning(self, text: str, result: DetectionResult, context: Dict[str, Any]):
        """Record detection result for continuous learning."""
        learning_record = {
            "timestamp": datetime.now().isoformat(),
            "text_hash": hashlib.md5(text.encode()).hexdigest(),
            "result": asdict(result),
            "context": context,
            "text_length": len(text)
        }
        
        self.training_data.append(learning_record)
        
        # Keep only recent training data (last 10000 records)
        if len(self.training_data) > 10000:
            self.training_data = self.training_data[-10000:]
    
    def add_feedback(self, text: str, actual_result: bool, user_feedback: str = ""):
        """Add user feedback for model improvement."""
        feedback_record = {
            "timestamp": datetime.now().isoformat(),
            "text_hash": hashlib.md5(text.encode()).hexdigest(),
            "actual_vulnerability": actual_result,
            "user_feedback": user_feedback
        }
        
        self.feedback_data.append(feedback_record)
        self.logger.info(f"Added feedback: vulnerability={actual_result}")
    
    def retrain_models(self) -> Dict[str, float]:
        """Retrain ML models with accumulated data."""
        if not ML_AVAILABLE or len(self.training_data) < 50:
            self.logger.warning("Insufficient data for retraining")
            return {}
        
        try:
            # Prepare training data
            X, y = self._prepare_training_data()
            
            # Retrain models
            results = {}
            for model_name, model in self.ml_models.items():
                if hasattr(model, 'fit'):
                    if model_name == "vulnerability_classifier":
                        scores = cross_val_score(model, X, y, cv=3)
                        model.fit(X, y)
                        results[model_name] = scores.mean()
            
            # Save updated models
            self._save_models()
            
            self.logger.info(f"Retrained {len(results)} models")
            return results
            
        except Exception as e:
            self.logger.error(f"Model retraining failed: {e}")
            return {}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for the detection system."""
        metrics = {
            "total_detections": len(self.training_data),
            "feedback_count": len(self.feedback_data),
            "pattern_count": len(self.vulnerability_patterns),
            "ml_enabled": ML_AVAILABLE
        }
        
        if self.feedback_data:
            # Calculate accuracy from feedback
            correct_predictions = sum(1 for f in self.feedback_data if f.get("actual_vulnerability", False))
            metrics["accuracy"] = correct_predictions / len(self.feedback_data)
        
        return metrics
    
    def _predict_vulnerability(self, features: Dict[str, float]) -> Dict[str, Any]:
        """Predict vulnerability using ML model."""
        # Simplified prediction - would use trained model in production
        feature_values = list(features.values())
        
        # Simple heuristic-based prediction
        vuln_score = (
            features.get("vuln_keyword_density", 0) * 0.4 +
            features.get("severity_keyword_count", 0) * 0.2 +
            features.get("tech_keyword_density", 0) * 0.2 +
            features.get("plugin_confidence", 0) * 0.2
        )
        
        return {
            "is_vulnerability": vuln_score > 0.3,
            "confidence": min(vuln_score, 1.0),
            "type": "UNKNOWN"
        }
    
    def _predict_false_positive(self, features: Dict[str, float]) -> float:
        """Predict false positive probability."""
        # Heuristic-based false positive detection
        fp_indicators = [
            features.get("text_length", 0) < 50,  # Very short text
            features.get("has_code_patterns", 0) == 0,  # No code patterns
            features.get("vuln_keyword_density", 0) > 0.5  # Too many vuln keywords
        ]
        
        return sum(fp_indicators) / len(fp_indicators)
    
    def _predict_severity(self, features: Dict[str, float]) -> str:
        """Predict vulnerability severity."""
        severity_score = features.get("severity_keyword_count", 0)
        
        if severity_score >= 3:
            return "HIGH"
        elif severity_score >= 1:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _prepare_training_data(self) -> Tuple[List[List[float]], List[int]]:
        """Prepare training data for ML models."""
        X = []
        y = []
        
        for record in self.training_data:
            features = record["result"]["ml_features"]
            is_vuln = record["result"]["is_vulnerability"]
            
            feature_vector = [
                features.get("text_length", 0),
                features.get("word_count", 0),
                features.get("vuln_keyword_density", 0),
                features.get("tech_keyword_density", 0),
                features.get("severity_keyword_count", 0),
                features.get("has_code_patterns", 0),
                features.get("plugin_confidence", 0)
            ]
            
            X.append(feature_vector)
            y.append(1 if is_vuln else 0)
        
        return X, y
    
    def _load_or_create_models(self):
        """Load existing models or create new ones."""
        model_file = self.model_cache_dir / "vulnerability_models.pkl"
        
        if model_file.exists():
            try:
                with open(model_file, 'rb') as f:
                    saved_data = pickle.load(f)
                    self.ml_models.update(saved_data.get("models", {}))
                    self.training_data = saved_data.get("training_data", [])
                    self.logger.info("Loaded existing ML models")
            except Exception as e:
                self.logger.warning(f"Failed to load models: {e}")
    
    def _save_models(self):
        """Save ML models to disk."""
        model_file = self.model_cache_dir / "vulnerability_models.pkl"
        
        try:
            save_data = {
                "models": self.ml_models,
                "training_data": self.training_data[-1000:],  # Keep recent data
                "timestamp": datetime.now().isoformat()
            }
            
            with open(model_file, 'wb') as f:
                pickle.dump(save_data, f)
                
            self.logger.info("Saved ML models to disk")
            
        except Exception as e:
            self.logger.error(f"Failed to save models: {e}") 