#!/usr/bin/env python3
"""
Vulnerability Data Validator

Comprehensive validation framework that ensures data consistency, catches duplicates,
validates logical relationships, and prevents nonsensical entries in security reports.
"""

import logging
import re
from typing import Dict, List, Any, Set, Tuple, Optional
from collections import defaultdict, Counter
from datetime import datetime
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Result of vulnerability data validation."""
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    statistics: Dict[str, Any]
    cleaned_data: Optional[List[Dict[str, Any]]] = None

class VulnerabilityDataValidator:
    """
    Comprehensive validator for vulnerability data that ensures:
    - Statistical consistency between summaries and details
    - No duplicate or nonsensical entries
    - Proper data types and ranges
    - Logical relationships between fields
    """
    
    def __init__(self):
        """Initialize the vulnerability data validator."""
        self.validation_rules = {
            'required_fields': ['title', 'severity', 'confidence'],
            'severity_levels': ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO', 'INFORMATIONAL'],
            'confidence_range': (0.0, 1.0),
            'risk_score_range': (0.0, 10.0),
            'valid_categories': [
                'general_security', 'permission_issue', 'network_security', 
                'debug_issue', 'injection', 'traversal', 'crypto', 'storage',
                'authentication', 'authorization', 'privacy', 'resilience'
            ]
        }
        
        # Patterns for nonsensical entries
        self.nonsensical_patterns = [
            r'^✅\s*(PASS|SUCCESS|OK)$',
            r'^❌\s*(FAIL|ERROR|FAILED)$',
            r'^(success|pass|ok|failed|error)$',
            r'^test\s*(passed|failed)$',
            r'^\s*$',  # Empty titles
            r'^unknown\s*(vulnerability)?$'
        ]
        
        # Patterns for status messages that shouldn't be vulnerabilities
        self.status_message_patterns = [
            r'analysis\s+(completed|finished|done)',
            r'scan\s+(completed|finished)',
            r'plugin\s+(executed|completed)',
            r'report\s+generated',
            r'configuration\s+(loaded|applied)'
        ]
        
        self.statistics = {
            'total_processed': 0,
            'duplicates_found': 0,
            'nonsensical_removed': 0,
            'data_errors_fixed': 0,
            'validation_errors': 0
        }
    
    def validate_vulnerability_dataset(self, vulnerabilities: List[Dict[str, Any]], 
                                     summary_stats: Dict[str, Any] = None) -> ValidationResult:
        """
        Perform comprehensive validation of vulnerability dataset.
        
        Args:
            vulnerabilities: List of vulnerability findings
            summary_stats: Optional summary statistics to validate against
            
        Returns:
            ValidationResult with validation outcome and cleaned data
        """
        logger.info(f"🔍 Validating dataset of {len(vulnerabilities)} vulnerabilities...")
        
        errors = []
        warnings = []
        
        # Reset statistics
        self.statistics = {key: 0 for key in self.statistics}
        self.statistics['total_processed'] = len(vulnerabilities)
        
        # Stage 1: Basic data validation
        validated_data = self._validate_basic_data_integrity(vulnerabilities, errors, warnings)
        
        # Stage 2: Remove nonsensical entries
        cleaned_data = self._remove_nonsensical_entries(validated_data, warnings)
        
        # Stage 3: Deduplicate entries
        deduplicated_data = self._deduplicate_vulnerabilities(cleaned_data, warnings)
        
        # Stage 4: Validate data consistency
        final_data = self._validate_data_consistency(deduplicated_data, errors, warnings)
        
        # Stage 5: Cross-validate with summary statistics
        if summary_stats:
            self._validate_summary_consistency(final_data, summary_stats, errors, warnings)
        
        # Stage 6: Validate logical relationships
        self._validate_logical_relationships(final_data, errors, warnings)
        
        # Generate final statistics
        final_stats = self._generate_validation_statistics(final_data)
        
        is_valid = len(errors) == 0
        
        logger.info(f"✅ Validation complete: {len(final_data)} valid vulnerabilities")
        logger.info(f"   Removed {self.statistics['nonsensical_removed']} nonsensical entries")
        logger.info(f"   Deduplicated {self.statistics['duplicates_found']} duplicates")
        logger.info(f"   Fixed {self.statistics['data_errors_fixed']} data errors")
        
        return ValidationResult(
            is_valid=is_valid,
            errors=errors,
            warnings=warnings,
            statistics=final_stats,
            cleaned_data=final_data
        )
    
    def _validate_basic_data_integrity(self, vulnerabilities: List[Dict[str, Any]], 
                                     errors: List[str], warnings: List[str]) -> List[Dict[str, Any]]:
        """Validate basic data integrity and fix common issues."""
        validated_data = []
        
        for i, vuln in enumerate(vulnerabilities):
            try:
                # Check required fields
                for field in self.validation_rules['required_fields']:
                    if field not in vuln or vuln[field] is None:
                        if field == 'title':
                            vuln[field] = f"Vulnerability {i+1}"
                            warnings.append(f"Missing title for vulnerability {i+1}, auto-generated")
                        elif field == 'severity':
                            vuln[field] = 'MEDIUM'
                            warnings.append(f"Missing severity for vulnerability {i+1}, defaulted to MEDIUM")
                        elif field == 'confidence':
                            vuln[field] = 0.7
                            warnings.append(f"Missing confidence for vulnerability {i+1}, defaulted to 0.7")
                        
                        self.statistics['data_errors_fixed'] += 1
                
                # Validate and normalize severity
                severity = str(vuln.get('severity', 'MEDIUM')).upper().strip()
                if severity not in self.validation_rules['severity_levels']:
                    # Try to map common variations
                    severity_mapping = {
                        'CRIT': 'CRITICAL', 'HI': 'HIGH', 'MED': 'MEDIUM', 
                        'LO': 'LOW', 'LOW': 'LOW', 'Information': 'INFO'
                    }
                    if severity in severity_mapping:
                        severity = severity_mapping[severity]
                        self.statistics['data_errors_fixed'] += 1
                    else:
                        warnings.append(f"Invalid severity '{vuln.get('severity')}' for vulnerability {i+1}, defaulted to MEDIUM")
                        severity = 'MEDIUM'
                        self.statistics['data_errors_fixed'] += 1
                
                vuln['severity'] = severity
                
                # Validate confidence range
                confidence = vuln.get('confidence', 0.7)
                try:
                    confidence = float(confidence)
                    if not (self.validation_rules['confidence_range'][0] <= confidence <= self.validation_rules['confidence_range'][1]):
                        if confidence > 1.0:
                            confidence = confidence / 100.0  # Convert percentage to decimal
                            self.statistics['data_errors_fixed'] += 1
                        else:
                            confidence = max(0.0, min(1.0, confidence))  # Clamp to valid range
                            warnings.append(f"Confidence out of range for vulnerability {i+1}, clamped to {confidence}")
                            self.statistics['data_errors_fixed'] += 1
                except (ValueError, TypeError):
                    confidence = 0.7
                    warnings.append(f"Invalid confidence value for vulnerability {i+1}, defaulted to 0.7")
                    self.statistics['data_errors_fixed'] += 1
                
                vuln['confidence'] = confidence
                
                # Validate risk score if present
                if 'risk_score' in vuln:
                    try:
                        risk_score = float(vuln['risk_score'])
                        if not (self.validation_rules['risk_score_range'][0] <= risk_score <= self.validation_rules['risk_score_range'][1]):
                            risk_score = max(0.0, min(10.0, risk_score))
                            warnings.append(f"Risk score out of range for vulnerability {i+1}, clamped to {risk_score}")
                            self.statistics['data_errors_fixed'] += 1
                        vuln['risk_score'] = risk_score
                    except (ValueError, TypeError):
                        del vuln['risk_score']
                        warnings.append(f"Invalid risk score for vulnerability {i+1}, removed")
                        self.statistics['data_errors_fixed'] += 1
                
                # Add unique ID if missing
                if 'id' not in vuln or not vuln['id']:
                    vuln['id'] = f"VULN-{i+1:04d}"
                    self.statistics['data_errors_fixed'] += 1
                
                # Normalize line numbers
                if 'line_number' in vuln:
                    try:
                        line_num = int(vuln['line_number'])
                        if line_num < 0:
                            line_num = 0
                        vuln['line_number'] = line_num
                    except (ValueError, TypeError):
                        vuln['line_number'] = 0
                        self.statistics['data_errors_fixed'] += 1
                
                validated_data.append(vuln)
                
            except Exception as e:
                errors.append(f"Critical validation error for vulnerability {i+1}: {e}")
                self.statistics['validation_errors'] += 1
        
        return validated_data
    
    def _remove_nonsensical_entries(self, vulnerabilities: List[Dict[str, Any]], 
                                  warnings: List[str]) -> List[Dict[str, Any]]:
        """Remove nonsensical entries like 'PASS', 'SUCCESS', etc."""
        cleaned_data = []
        
        for vuln in vulnerabilities:
            title = str(vuln.get('title', '')).strip()
            description = str(vuln.get('description', '')).strip()
            
            # Check for nonsensical title patterns
            is_nonsensical = False
            
            for pattern in self.nonsensical_patterns:
                if re.match(pattern, title, re.IGNORECASE):
                    is_nonsensical = True
                    break
            
            # Check for status message patterns
            if not is_nonsensical:
                combined_text = f"{title} {description}".lower()
                for pattern in self.status_message_patterns:
                    if re.search(pattern, combined_text, re.IGNORECASE):
                        is_nonsensical = True
                        break
            
            # Check for extremely low confidence with generic titles
            if not is_nonsensical:
                confidence = vuln.get('confidence', 1.0)
                if confidence < 0.3 and any(word in title.lower() for word in ['unknown', 'generic', 'test', 'placeholder']):
                    is_nonsensical = True
            
            if is_nonsensical:
                warnings.append(f"Removed nonsensical entry: '{title}'")
                self.statistics['nonsensical_removed'] += 1
            else:
                cleaned_data.append(vuln)
        
        return cleaned_data
    
    def _deduplicate_vulnerabilities(self, vulnerabilities: List[Dict[str, Any]], 
                                   warnings: List[str]) -> List[Dict[str, Any]]:
        """Remove duplicate vulnerabilities using fingerprinting."""
        
        def create_fingerprint(vuln: Dict[str, Any]) -> str:
            """Create a unique fingerprint for a vulnerability."""
            # Primary fingerprint based on location and type
            file_path = vuln.get('file_path', '').strip()
            line_number = vuln.get('line_number', 0)
            vuln_type = vuln.get('vulnerability_type', vuln.get('category', '')).strip().lower()
            title = vuln.get('title', '').strip().lower()
            
            # Normalize title for comparison
            normalized_title = re.sub(r'[^\w\s]', '', title)
            normalized_title = re.sub(r'\s+', ' ', normalized_title).strip()
            
            # Create fingerprint
            if file_path and line_number > 0:
                # Location-based fingerprint for code-level findings
                return f"{file_path}:{line_number}:{vuln_type}"
            else:
                # Content-based fingerprint for general findings
                return f"{normalized_title}:{vuln_type}"
        
        fingerprint_to_vuln = {}
        duplicates_found = 0
        
        for vuln in vulnerabilities:
            fingerprint = create_fingerprint(vuln)
            
            if fingerprint in fingerprint_to_vuln:
                # Duplicate found - merge with existing
                existing = fingerprint_to_vuln[fingerprint]
                
                # Keep the one with higher confidence
                if vuln.get('confidence', 0) > existing.get('confidence', 0):
                    fingerprint_to_vuln[fingerprint] = vuln
                    warnings.append(f"Merged duplicate: '{vuln.get('title')}' (kept higher confidence)")
                else:
                    warnings.append(f"Removed duplicate: '{vuln.get('title')}'")
                
                duplicates_found += 1
                self.statistics['duplicates_found'] += 1
            else:
                fingerprint_to_vuln[fingerprint] = vuln
        
        deduplicated_data = list(fingerprint_to_vuln.values())
        
        if duplicates_found > 0:
            warnings.append(f"Removed {duplicates_found} duplicate vulnerabilities")
        
        return deduplicated_data
    
    def _validate_data_consistency(self, vulnerabilities: List[Dict[str, Any]], 
                                 errors: List[str], warnings: List[str]) -> List[Dict[str, Any]]:
        """Validate internal data consistency within each vulnerability."""
        consistent_data = []
        
        for vuln in vulnerabilities:
            # Validate severity vs risk_score consistency
            if 'risk_score' in vuln:
                severity = vuln.get('severity', 'MEDIUM')
                risk_score = vuln.get('risk_score', 5.0)
                
                # Expected risk score ranges for severity levels
                severity_ranges = {
                    'CRITICAL': (8.5, 10.0),
                    'HIGH': (6.5, 8.5),
                    'MEDIUM': (4.0, 6.5),
                    'LOW': (2.0, 4.0),
                    'INFO': (0.0, 2.0),
                    'INFORMATIONAL': (0.0, 2.0)
                }
                
                expected_range = severity_ranges.get(severity, (0.0, 10.0))
                if not (expected_range[0] <= risk_score <= expected_range[1]):
                    # Adjust severity to match risk score
                    for sev, (min_score, max_score) in severity_ranges.items():
                        if min_score <= risk_score <= max_score:
                            vuln['severity'] = sev
                            warnings.append(f"Adjusted severity from {severity} to {sev} to match risk score {risk_score}")
                            self.statistics['data_errors_fixed'] += 1
                            break
            
            # Validate MASVS controls format
            if 'masvs_controls' in vuln:
                controls = vuln['masvs_controls']
                if isinstance(controls, str):
                    # Convert string to list
                    controls = [c.strip() for c in controls.split(',')]
                    vuln['masvs_controls'] = controls
                    self.statistics['data_errors_fixed'] += 1
                elif not isinstance(controls, list):
                    del vuln['masvs_controls']
                    warnings.append(f"Invalid MASVS controls format for '{vuln.get('title')}', removed")
                    self.statistics['data_errors_fixed'] += 1
                else:
                    # Validate control format
                    valid_controls = []
                    for control in controls:
                        if isinstance(control, str) and re.match(r'^MASVS-[A-Z]+-\d+$', control.strip()):
                            valid_controls.append(control.strip())
                    
                    if len(valid_controls) != len(controls):
                        vuln['masvs_controls'] = valid_controls
                        warnings.append(f"Filtered invalid MASVS controls for '{vuln.get('title')}'")
                        self.statistics['data_errors_fixed'] += 1
            
            # Validate file path and line number consistency
            file_path = vuln.get('file_path', '')
            line_number = vuln.get('line_number', 0)
            
            if file_path and not line_number:
                warnings.append(f"File path without line number for '{vuln.get('title')}'")
            elif line_number and not file_path:
                warnings.append(f"Line number without file path for '{vuln.get('title')}'")
                vuln['line_number'] = 0  # Clear invalid line number
                self.statistics['data_errors_fixed'] += 1
            
            consistent_data.append(vuln)
        
        return consistent_data
    
    def _validate_summary_consistency(self, vulnerabilities: List[Dict[str, Any]], 
                                    summary_stats: Dict[str, Any],
                                    errors: List[str], warnings: List[str]):
        """Validate that summary statistics match the detailed findings."""
        
        # Count actual vulnerabilities by severity
        actual_counts = Counter(vuln.get('severity', 'UNKNOWN') for vuln in vulnerabilities)
        
        # Check claimed vs actual counts
        if 'severity_breakdown' in summary_stats:
            claimed_counts = summary_stats['severity_breakdown']
            
            for severity, claimed_count in claimed_counts.items():
                actual_count = actual_counts.get(severity.upper(), 0)
                if claimed_count != actual_count:
                    errors.append(f"Summary mismatch: {severity} claimed {claimed_count}, found {actual_count}")
        
        # Check total vulnerability count
        if 'total_vulnerabilities' in summary_stats:
            claimed_total = summary_stats['total_vulnerabilities']
            actual_total = len(vulnerabilities)
            if claimed_total != actual_total:
                errors.append(f"Total count mismatch: claimed {claimed_total}, found {actual_total}")
        
        # Check high confidence findings
        high_confidence_count = sum(1 for v in vulnerabilities if v.get('confidence', 0) >= 0.9)
        if 'high_confidence_findings' in summary_stats:
            claimed_high_conf = summary_stats['high_confidence_findings']
            if claimed_high_conf != high_confidence_count:
                warnings.append(f"High confidence count mismatch: claimed {claimed_high_conf}, found {high_confidence_count}")
        
        # Check code-level findings
        code_level_count = sum(1 for v in vulnerabilities if v.get('file_path') and v.get('line_number', 0) > 0)
        if 'code_level_findings' in summary_stats:
            claimed_code_level = summary_stats['code_level_findings']
            if claimed_code_level != code_level_count:
                warnings.append(f"Code-level count mismatch: claimed {claimed_code_level}, found {code_level_count}")
    
    def _validate_logical_relationships(self, vulnerabilities: List[Dict[str, Any]], 
                                      errors: List[str], warnings: List[str]):
        """Validate logical relationships between fields."""
        
        for vuln in vulnerabilities:
            # Check confidence vs severity relationship
            confidence = vuln.get('confidence', 0.7)
            severity = vuln.get('severity', 'MEDIUM')
            
            # Critical findings should generally have high confidence
            if severity == 'CRITICAL' and confidence < 0.7:
                warnings.append(f"Critical severity with low confidence ({confidence:.1%}) for '{vuln.get('title')}'")
            
            # Low confidence findings shouldn't be high severity
            if confidence < 0.5 and severity in ['CRITICAL', 'HIGH']:
                warnings.append(f"High severity with very low confidence ({confidence:.1%}) for '{vuln.get('title')}'")
            
            # Check MASVS controls vs category consistency
            masvs_controls = vuln.get('masvs_controls', [])
            category = vuln.get('category', '').lower()
            
            if masvs_controls and category:
                # Extract MASVS categories from controls
                control_categories = set()
                for control in masvs_controls:
                    if '-' in control:
                        parts = control.split('-')
                        if len(parts) >= 2:
                            control_categories.add(parts[1].lower())
                
                # Check for obvious mismatches
                category_mappings = {
                    'network': 'network',
                    'crypto': 'crypto',
                    'storage': 'storage',
                    'auth': 'auth',
                    'platform': 'platform'
                }
                
                expected_category = category_mappings.get(category)
                if expected_category and expected_category not in control_categories:
                    warnings.append(f"MASVS controls don't match category for '{vuln.get('title')}'")
    
    def _generate_validation_statistics(self, vulnerabilities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive validation statistics."""
        
        severity_counts = Counter(vuln.get('severity', 'UNKNOWN') for vuln in vulnerabilities)
        confidence_ranges = {
            'high_confidence': sum(1 for v in vulnerabilities if v.get('confidence', 0) >= 0.9),
            'medium_confidence': sum(1 for v in vulnerabilities if 0.7 <= v.get('confidence', 0) < 0.9),
            'low_confidence': sum(1 for v in vulnerabilities if v.get('confidence', 0) < 0.7)
        }
        
        masvs_stats = {
            'vulnerabilities_with_masvs': sum(1 for v in vulnerabilities if v.get('masvs_controls')),
            'unique_masvs_controls': len(set().union(*[v.get('masvs_controls', []) for v in vulnerabilities])),
            'masvs_coverage_percentage': (sum(1 for v in vulnerabilities if v.get('masvs_controls')) / len(vulnerabilities) * 100) if vulnerabilities else 0
        }
        
        location_stats = {
            'with_exact_location': sum(1 for v in vulnerabilities if v.get('file_path') and v.get('line_number', 0) > 0),
            'with_file_path': sum(1 for v in vulnerabilities if v.get('file_path')),
            'with_code_context': sum(1 for v in vulnerabilities if v.get('matching_code'))
        }
        
        return {
            'total_vulnerabilities': len(vulnerabilities),
            'severity_breakdown': dict(severity_counts),
            'confidence_breakdown': confidence_ranges,
            'masvs_statistics': masvs_stats,
            'location_statistics': location_stats,
            'validation_statistics': self.statistics.copy(),
            'data_quality_score': self._calculate_data_quality_score(vulnerabilities)
        }
    
    def _calculate_data_quality_score(self, vulnerabilities: List[Dict[str, Any]]) -> float:
        """Calculate overall data quality score (0-100)."""
        if not vulnerabilities:
            return 0.0
        
        total_score = 0.0
        total_weight = 0.0
        
        # Weight factors for different quality aspects
        weights = {
            'has_required_fields': 30.0,
            'has_valid_confidence': 20.0,
            'has_exact_location': 15.0,
            'has_masvs_controls': 15.0,
            'has_description': 10.0,
            'has_remediation': 10.0
        }
        
        for vuln in vulnerabilities:
            vuln_score = 0.0
            
            # Required fields completeness
            required_complete = all(vuln.get(field) for field in ['title', 'severity', 'confidence'])
            if required_complete:
                vuln_score += weights['has_required_fields']
            
            # Confidence validity
            confidence = vuln.get('confidence', 0)
            if 0.7 <= confidence <= 1.0:
                vuln_score += weights['has_valid_confidence']
            elif 0.5 <= confidence < 0.7:
                vuln_score += weights['has_valid_confidence'] * 0.7
            elif confidence >= 0.3:
                vuln_score += weights['has_valid_confidence'] * 0.3
            
            # Location information
            if vuln.get('file_path') and vuln.get('line_number', 0) > 0:
                vuln_score += weights['has_exact_location']
            elif vuln.get('file_path'):
                vuln_score += weights['has_exact_location'] * 0.5
            
            # MASVS controls
            if vuln.get('masvs_controls'):
                vuln_score += weights['has_masvs_controls']
            
            # Description quality
            description = vuln.get('description', '')
            if description and len(description) > 20:
                vuln_score += weights['has_description']
            elif description:
                vuln_score += weights['has_description'] * 0.5
            
            # Remediation guidance
            if vuln.get('remediation_guidance') or vuln.get('remediation'):
                vuln_score += weights['has_remediation']
            
            total_score += vuln_score
            total_weight += sum(weights.values())
        
        return (total_score / total_weight) * 100 if total_weight > 0 else 0.0

def validate_vulnerability_data(vulnerabilities: List[Dict[str, Any]], 
                              summary_stats: Dict[str, Any] = None) -> ValidationResult:
    """Convenience function for validating vulnerability data."""
    validator = VulnerabilityDataValidator()
    return validator.validate_vulnerability_dataset(vulnerabilities, summary_stats) 